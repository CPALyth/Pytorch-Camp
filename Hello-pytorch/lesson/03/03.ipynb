{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1e2cd04ee90>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t_0 shape:torch.Size([4, 3])\n",
      "t_1:tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "t_1 shape:torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "# cat直接拼接\n",
    "t = torch.ones((2,3))\n",
    "t_0 = torch.cat([t,t], dim=0)\n",
    "t_1 = torch.cat([t,t,t], dim=1)\n",
    "print(\"t_0:{}\".format(t_0))\n",
    "print(\"t_0 shape:{}\".format(t_0.shape))\n",
    "print(\"t_1:{}\".format(t_1))\n",
    "print(\"t_1 shape:{}\".format(t_1.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_stack:tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "t_stack shape:torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# stack扩维拼接\n",
    "t = torch.ones((2,3))\n",
    "t_stack = torch.stack([t,t,t], dim=0)\n",
    "print(\"t_stack:{}\".format(t_stack))\n",
    "print(\"t_stack shape:{}\".format(t_stack.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个张量:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第2个张量:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第3个张量:tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# chunk均分为n块\n",
    "a = torch.ones((2, 7))\n",
    "list_of_tensors = torch.chunk(a, dim=1, chunks=3)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量:{}, shape is {}\".format(idx+1, t, t.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个张量:tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n",
      "第2个张量:tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n",
      "第3个张量:tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# split按指定规则分割\n",
    "a = torch.ones((2, 5))\n",
    "list_of_tensors = torch.split(a, [2,1,2], dim=1)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量:{}, shape is {}\".format(idx+1, t, t.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:\n",
      "tensor([[4, 5, 0],\n",
      "        [5, 7, 1],\n",
      "        [2, 5, 8]])\n",
      "\n",
      "t_select:\n",
      "tensor([[4, 5, 0],\n",
      "        [2, 5, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index_select选择tensor中的某几行或列\n",
    "t = torch.randint(0, 9, size=(3,3))\n",
    "idx = torch.tensor([0,2], dtype=torch.long)\n",
    "t_select = torch.index_select(t, dim=0, index=idx)\n",
    "print(\"t:\\n{}\\n\".format(t))\n",
    "print(\"t_select:\\n{}\\n\".format(t_select))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:\n",
      "tensor([[0, 2, 3],\n",
      "        [1, 8, 4],\n",
      "        [0, 3, 6]])\n",
      " mask:\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True],\n",
      "        [ True,  True, False]])\n",
      " t_sel:\n",
      "tensor([0, 2, 3, 1, 4, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "# masked_select获取tensor中满足mask条件\n",
    "t = torch.randint(0, 9, size=(3,3))\n",
    "mask = t.le(5)\n",
    "t_select = torch.masked_select(t, mask)\n",
    "print(\"t:\\n{}\\n mask:\\n{}\\n t_sel:\\n{}\".format(t, mask, t_select))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([2, 0, 1, 6, 3, 4, 7, 5])\n",
      " t_reshape:tensor([[[2, 0],\n",
      "         [1, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 5]]])\n",
      "\n",
      "t:tensor([1024,    0,    1,    6,    3,    4,    7,    5])\n",
      "t_reshape:tensor([[[1024,    0],\n",
      "         [   1,    6]],\n",
      "\n",
      "        [[   3,    4],\n",
      "         [   7,    5]]])\n",
      "t.data内存地址:2073793815240\n",
      "t_reshape.data内存地址:2073793815240\n"
     ]
    }
   ],
   "source": [
    "# reshape改变原tensor的形状\n",
    "t = torch.randperm(8)\n",
    "t_reshape = torch.reshape(t, (-1,2,2))\n",
    "print(\"t:{}\\n t_reshape:{}\\n\".format(t, t_reshape))\n",
    "t[0] = 1024\n",
    "print(\"t:{}\\nt_reshape:{}\".format(t, t_reshape))\n",
    "print(\"t.data内存地址:{}\".format(id(t.data)))\n",
    "print((\"t_reshape.data内存地址:{}\").format(id(t_reshape.data)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([[[0.5239, 0.7981, 0.7718, 0.0112],\n",
      "         [0.8100, 0.6397, 0.9743, 0.8300],\n",
      "         [0.0444, 0.0246, 0.2588, 0.9391]],\n",
      "\n",
      "        [[0.4167, 0.7140, 0.2676, 0.9906],\n",
      "         [0.2885, 0.8750, 0.5059, 0.2366],\n",
      "         [0.7570, 0.2346, 0.6471, 0.3556]]])\n",
      "t_transpose:tensor([[[0.5239, 0.8100, 0.0444],\n",
      "         [0.7981, 0.6397, 0.0246],\n",
      "         [0.7718, 0.9743, 0.2588],\n",
      "         [0.0112, 0.8300, 0.9391]],\n",
      "\n",
      "        [[0.4167, 0.2885, 0.7570],\n",
      "         [0.7140, 0.8750, 0.2346],\n",
      "         [0.2676, 0.5059, 0.6471],\n",
      "         [0.9906, 0.2366, 0.3556]]])\n",
      "t shape:torch.Size([2, 3, 4])\n",
      " t_transpose shape:torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# transpose对指定的两个维度进行转置\n",
    "t = torch.rand((2,3,4))\n",
    "t_transpose = torch.transpose(t, dim0=1, dim1=2)\n",
    "print(\"t:{}\\nt_transpose:{}\".format(t, t_transpose))\n",
    "print(\"t shape:{}\\n t_transpose shape:{}\".format(t.shape, t_transpose.shape))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# squeeze压缩tensor中为1的维度,实现降维\n",
    "t = torch.rand((1,2,3,1))\n",
    "t_sq = torch.squeeze(t)\n",
    "t_0 = torch.squeeze(t, dim=0)\n",
    "t_1 = torch.squeeze(t, dim=1)\n",
    "print(t.shape)\n",
    "print(t_sq.shape)\n",
    "print(t_0.shape)\n",
    "print(t_1.shape) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0:\n",
      "tensor([[ 0.2734,  2.0731, -1.9578],\n",
      "        [ 2.8238,  0.2384, -0.6317],\n",
      "        [ 1.3790,  0.6429, -0.4019]])\n",
      "t_1:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t_add:\n",
      "tensor([[10.2734, 12.0731,  8.0422],\n",
      "        [12.8238, 10.2384,  9.3683],\n",
      "        [11.3790, 10.6429,  9.5981]])\n"
     ]
    }
   ],
   "source": [
    "# add实现两个维度相同的tensor相加\n",
    "t_0 = torch.randn((3,3))\n",
    "t_1 = torch.ones_like(t_0)\n",
    "t_add = torch.add(t_0, t_1, alpha=10)\n",
    "print(\"t_0:\\n{}\\nt_1:\\n{}\\nt_add:\\n{}\".format(t_0, t_1, t_add))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "PyTorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}