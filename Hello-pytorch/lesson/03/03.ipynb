{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x22026b0ee50>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t_0 shape:torch.Size([4, 3])\n",
      "t_1:tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "t_1 shape:torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "# cat直接拼接\n",
    "t = torch.ones((2,3))\n",
    "t_0 = torch.cat([t,t], dim=0)\n",
    "t_1 = torch.cat([t,t,t], dim=1)\n",
    "print(\"t_0:{}\".format(t_0))\n",
    "print(\"t_0 shape:{}\".format(t_0.shape))\n",
    "print(\"t_1:{}\".format(t_1))\n",
    "print(\"t_1 shape:{}\".format(t_1.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_stack:tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "t_stack shape:torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# stack扩维拼接\n",
    "t = torch.ones((2,3))\n",
    "t_stack = torch.stack([t,t,t], dim=0)\n",
    "print(\"t_stack:{}\".format(t_stack))\n",
    "print(\"t_stack shape:{}\".format(t_stack.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个张量:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第2个张量:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]), shape is torch.Size([2, 3])\n",
      "第3个张量:tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# chunk均分为n块\n",
    "a = torch.ones((2, 7))\n",
    "list_of_tensors = torch.chunk(a, dim=1, chunks=3)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量:{}, shape is {}\".format(idx+1, t, t.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个张量:tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n",
      "第2个张量:tensor([[1.],\n",
      "        [1.]]), shape is torch.Size([2, 1])\n",
      "第3个张量:tensor([[1., 1.],\n",
      "        [1., 1.]]), shape is torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# split按指定规则分割\n",
    "a = torch.ones((2, 5))\n",
    "list_of_tensors = torch.split(a, [2,1,2], dim=1)\n",
    "for idx, t in enumerate(list_of_tensors):\n",
    "    print(\"第{}个张量:{}, shape is {}\".format(idx+1, t, t.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:\n",
      "tensor([[0, 2, 3],\n",
      "        [1, 8, 4],\n",
      "        [0, 3, 6]])\n",
      "\n",
      "t_select:\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [0, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index_select选择tensor中的某几行或列\n",
    "t = torch.randint(0, 9, size=(3,3))\n",
    "idx = torch.tensor([0,2], dtype=torch.long)\n",
    "t_select = torch.index_select(t, dim=1, index=idx)\n",
    "print(\"t:\\n{}\\n\".format(t))\n",
    "print(\"t_select:\\n{}\\n\".format(t_select))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:\n",
      "tensor([[0, 2, 3],\n",
      "        [1, 8, 4],\n",
      "        [0, 3, 6]])\n",
      " mask:\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True],\n",
      "        [ True,  True, False]])\n",
      " t_sel:\n",
      "tensor([0, 2, 3, 1, 4, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "# masked_select获取tensor中满足mask条件\n",
    "t = torch.randint(0, 9, size=(3,3))\n",
    "mask = t.le(5)\n",
    "t_select = torch.masked_select(t, mask)\n",
    "print(\"t:\\n{}\\n mask:\\n{}\\n t_sel:\\n{}\".format(t, mask, t_select))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([7, 0, 3, 6, 1, 5, 2, 4])\n",
      " t_reshape:tensor([[[7, 0],\n",
      "         [3, 6]],\n",
      "\n",
      "        [[1, 5],\n",
      "         [2, 4]]])\n",
      "\n",
      "t:tensor([1024,    0,    3,    6,    1,    5,    2,    4])\n",
      "t_reshape:tensor([[[1024,    0],\n",
      "         [   3,    6]],\n",
      "\n",
      "        [[   1,    5],\n",
      "         [   2,    4]]])\n",
      "t.data内存地址:2121002189752\n",
      "t_reshape.data内存地址:2121002189752\n"
     ]
    }
   ],
   "source": [
    "# reshape改变原tensor的形状\n",
    "t = torch.randperm(8)\n",
    "t_reshape = torch.reshape(t, (-1,2,2))\n",
    "print(\"t:{}\\n t_reshape:{}\\n\".format(t, t_reshape))\n",
    "t[0] = 1024\n",
    "print(\"t:{}\\nt_reshape:{}\".format(t, t_reshape))\n",
    "print(\"t.data内存地址:{}\".format(id(t.data)))\n",
    "print((\"t_reshape.data内存地址:{}\").format(id(t_reshape.data)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8,  2, 13, 15,  6,  3,  9, 14, 11,  4,  7,  5,  0, 12, 10,  1]) \n",
      "tensor([[[[ 8,  2],\n",
      "          [13, 15]],\n",
      "\n",
      "         [[ 6,  3],\n",
      "          [ 9, 14]]],\n",
      "\n",
      "\n",
      "        [[[11,  4],\n",
      "          [ 7,  5]],\n",
      "\n",
      "         [[ 0, 12],\n",
      "          [10,  1]]]])\n",
      "2337146075880 \n",
      "2337142570664\n",
      "tensor([1024,    2,   13,   15,    6,    3,    9,   14,   11,    4,    7,    5,\n",
      "           0,   12,   10,    1])\n",
      "tensor([[[[1024,    2],\n",
      "          [  13,   15]],\n",
      "\n",
      "         [[   6,    3],\n",
      "          [   9,   14]]],\n",
      "\n",
      "\n",
      "        [[[  11,    4],\n",
      "          [   7,    5]],\n",
      "\n",
      "         [[   0,   12],\n",
      "          [  10,    1]]]])\n",
      "2337146075880\n",
      "2337142570664\n"
     ]
    }
   ],
   "source": [
    "t = torch.randperm(16)\n",
    "t_view = t.view(-1, 2,2,2)\n",
    "print(f\"{t} \\n{t_view}\")\n",
    "print(f\"{id(t)} \\n{id(t_view)}\")\n",
    "t[0] = 1024\n",
    "print(f\"{t}\\n{t_view}\")\n",
    "print(f\"{id(t)}\\n{id(t_view)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:tensor([[[0.2676, 0.9906, 0.2885, 0.8750],\n",
      "         [0.5059, 0.2366, 0.7570, 0.2346],\n",
      "         [0.6471, 0.3556, 0.4452, 0.0193]],\n",
      "\n",
      "        [[0.2616, 0.7713, 0.3785, 0.9980],\n",
      "         [0.9008, 0.4766, 0.1663, 0.8045],\n",
      "         [0.6552, 0.1768, 0.8248, 0.8036]]])\n",
      "t_transpose:tensor([[[0.2676, 0.5059, 0.6471],\n",
      "         [0.9906, 0.2366, 0.3556],\n",
      "         [0.2885, 0.7570, 0.4452],\n",
      "         [0.8750, 0.2346, 0.0193]],\n",
      "\n",
      "        [[0.2616, 0.9008, 0.6552],\n",
      "         [0.7713, 0.4766, 0.1768],\n",
      "         [0.3785, 0.1663, 0.8248],\n",
      "         [0.9980, 0.8045, 0.8036]]])\n",
      "t shape:torch.Size([2, 3, 4])\n",
      " t_transpose shape:torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# transpose对指定的两个维度进行转置\n",
    "t = torch.rand((2,3,4))\n",
    "t_transpose = torch.transpose(t, dim0=1, dim1=2)\n",
    "print(\"t:{}\\nt_transpose:{}\".format(t, t_transpose))\n",
    "print(\"t shape:{}\\n t_transpose shape:{}\".format(t.shape, t_transpose.shape))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# squeeze压缩tensor中为1的维度,实现降维\n",
    "t = torch.rand((1,2,3,1))\n",
    "t_sq = torch.squeeze(t)\n",
    "t_0 = torch.squeeze(t, dim=0)\n",
    "t_1 = torch.squeeze(t, dim=1)\n",
    "print(t.shape)\n",
    "print(t_sq.shape)\n",
    "print(t_0.shape)\n",
    "print(t_1.shape) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_0:\n",
      "tensor([[-0.0179,  0.1280, -0.5552],\n",
      "        [-0.4575, -1.9599, -1.1242],\n",
      "        [-1.5599,  1.4003,  0.2963]])\n",
      "t_1:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "t_add:\n",
      "tensor([[ 0.9821,  1.1280,  0.4448],\n",
      "        [ 0.5425, -0.9599, -0.1242],\n",
      "        [-0.5599,  2.4003,  1.2963]])\n"
     ]
    }
   ],
   "source": [
    "# add实现两个维度相同的tensor相加\n",
    "t_0 = torch.randn((3,3), dtype=torch.float)\n",
    "t_1 = torch.ones_like(t_0)\n",
    "t_add = torch.add(t_0, t_1)\n",
    "print(\"t_0:\\n{}\\nt_1:\\n{}\\nt_add:\\n{}\".format(t_0, t_1, t_add))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "PyTorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}